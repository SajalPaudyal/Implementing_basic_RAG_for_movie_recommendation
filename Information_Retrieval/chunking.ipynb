{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61ada9b",
   "metadata": {},
   "source": [
    "# The process of creating a RAG and an integral part of it - Chunking\n",
    "\n",
    "Dividing text into chunks before embedding is a very impactful and integral step as it dictates what information is included in the vector and then is found during the search. Chunks should be of appropriate size as too small chunks lose context and that which are too big are non-specific which ultimately impacts the retrieval of query specific information. Excessive context might also lead to hallucination and deduce the LLM performance. Also, the chunk size should not exceed the context length of the embedder or else we will lose the information - this is called **truncation**. Therefore chunk size is important and it affects both the quality and performance of both retrieval and generation. \n",
    "\n",
    "Simplest strategy is to **fix the length of the chunk**. Character chunking divides the document into chunks based on predetermined number of characters of tokens (common choices are 100 or 256 tokens, or 500 characters). This size should be chosen depending on the type of the document. This is the cheapest and easiest system to implement. \n",
    "\n",
    "We can implement **random size chunking** when the collection is non homogeneous and can potentially capture more semantic context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851cee1",
   "metadata": {},
   "source": [
    "## Chunking without overlap\n",
    "\n",
    "for example, \n",
    "```\n",
    "\"Nepal is a landlocked country and therefore it has no access to ocean making it difficult to import electronic goods from china directly.\"\n",
    "\n",
    "is divided into \n",
    "\n",
    "Chunk 1: Nepal is a landlocked country\n",
    "Chunk 2: and therefore it has no access\n",
    "Chunk 3: to ocean making it difficult to\n",
    "Chunk 4: import electronic goods from China directly\n",
    "```\n",
    "\n",
    "It works well when there are clear boundaries between chunks, for example if the context drastically changes between adjacent chunks. But this is rare and hence the lack of overlap destroys context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7282c32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sajalpaudyal/Documents/Programming_Works/basic_rag/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from IPython.display import HTML\n",
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8a7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size =50, \n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "772b07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_text_chunks(text, text_splitter):\n",
    "    docs = text_splitter.create_documents([text])\n",
    "    chunks = [doc.page_content for doc in docs]\n",
    "    \n",
    "    colored_text = \"\"\n",
    "    colors = [\"#ff9999\",\"#66ffff\", \"#99ff99\", \"#ff66ff\"]\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        color = colors[i % len(colors)]\n",
    "        chunk_html = f\"<span style='background-color: {color};color:black'>{chunk}</span>\"\n",
    "        colored_text += chunk_html + \"<br/>\"\n",
    "    \n",
    "    return HTML(colored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e320e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemmingway = \"\"\"\n",
    "Today is only one day in all the days that will ever be. But what will happen in all the other days that ever come can depend on what you do today. It's been that way all this year. It's been that way so many times. All of war is that way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99cc69a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='background-color: #ff9999;color:black'>\n",
       "Today is only one day in all the days that will ever be. But what will happen in all the other days that ever come can depend on what you do today. It's been that way all this year. It's been that way so</span><br/><span style='background-color: #66ffff;color:black'> many times. All of war is that way.\n",
       "</span><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_text_chunks(hemmingway, text_splitter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
